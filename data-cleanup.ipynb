{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions From Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Conclusion\n",
    "\n",
    "| Column | Values Removed | Reason |\n",
    "| :-- | :-: | :-- |\n",
    "| All Column | values <= 0 | These are either impossible (e.g. negative amount of sludge in water like `InpA Sludge In Water \\[mg/l\\]`) or unwanted (e.g. maintenances) |\n",
    "| Tank1 Content Height | values > 25 m | The tank is probably not higher than 25m |\n",
    "| Tank2 Content Height | values > 25 m | The tank is probably not higher than 25m |\n",
    "| Tank1 Sludge Recycle In Flow | values > 150 m3/h | Considering that the InpA and InpB flow rates are never higher than 150 m3/h, it seems that values for `Tank1 Sludge Recycle In Flow` higher than 150 m3/h are outliers | \n",
    "| Tank2 Sludge Recycle In Flow | values > 200 m3/h | Same reasoning as above | \n",
    "| Exit N03 Dissolved | drop column | Too many missing values (56%) |\n",
    "| Target | All NaN | Remove all rows without a target, they cant be used for modeling |\n",
    "| All | 40% missing variables | Remove rows with more than 40% of the row missing |\n",
    "\n",
    "\n",
    "### Additional Changes \n",
    "- Rows with at least one negative variable are removed entirely, as these are periods that we don't want to consider. The other removals shown in the table above are considered outliers. We set these outlying values to 'NaN'. Gaps of NaN that are not too long will be interpolated from neighboring 'good' values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = dataset.copy()\n",
    "# Remove rows where at least one variable is negative or zero\n",
    "clean_data = clean_data.loc[~(clean_data <= 0).any(axis=1)]\n",
    "print(f\"Removing rows with at least one nonpositive variable: {dataset.shape[0] - clean_data.shape[0]} rows were removed. \")\n",
    "\n",
    "# Handle the tank1 content height outliers:\n",
    "remove = (clean_data[('Tank 1', 'Content height')] > 25)\n",
    "clean_data.loc[remove] = np.nan\n",
    "print(f\"Setting values with Tank 1 Content height > 25m to NaN: {remove.sum()} observations were affected.\")\n",
    "\n",
    "# Handle the tank2 content height outliers:\n",
    "remove = (clean_data[('Tank 2', 'Content height')] > 25)\n",
    "clean_data.loc[remove] = np.nan\n",
    "print(f\"Setting values with Tank 2 Content height > 25m to NaN: {remove.sum()} observations were affected.\")\n",
    "\n",
    "# Handle the Tank1 Sludge Recycle In Flow outliers:\n",
    "remove = (clean_data[('Tank 1', 'Sludge recycle in flow')] > 150)\n",
    "clean_data.loc[remove] = np.nan\n",
    "print(f\"Setting values with Tank 1 Sludge recycle in flow > 150m to NaN: {remove.sum()} observations were affected.\")\n",
    "\n",
    "# Handle the Tank2 Sludge Recycle In Flow outliers:\n",
    "remove = (clean_data[('Tank 2', 'Sludge recycle in flow')] > 200)\n",
    "clean_data.loc[remove] = np.nan\n",
    "print(f\"Setting values with Tank 2 Sludge recycle in flow > 200m to NaN: {remove.sum()} observations were affected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = clean_data[('Exit', 'Target')].isna()\n",
    "clean_data = clean_data.loc[~mask]\n",
    "\n",
    "print(f'{mask.sum()} timestamps of the Target variable are missing. These full rows are removed. {clean_data.shape[0]} rows remain.')\n",
    "\n",
    "threshold = .4 # 40% missing variables in one timestamp\n",
    "\n",
    "missings = clean_data.isna().sum(axis=1) / clean_data.shape[1]\n",
    "\n",
    "clean_data = clean_data.loc[missings < threshold]\n",
    "print(f'{(missings >= threshold).sum()} rows had more missing values than the threshold. {clean_data.shape[0]} rows remain.')\n",
    "\n",
    "# Re-make the grouped_data and group-names\n",
    "grouped_data =clean_data.groupby(level=1, axis=1)\n",
    "# Unique group names, sorted\n",
    "group_names = sorted(grouped_data.groups.keys(), key=str.casefold)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
